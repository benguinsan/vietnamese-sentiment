{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tDr26VcDaZZd",
        "outputId": "d5f2456f-ea21-4993-93a0-6da5172f3a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in ./.venv/lib/python3.11/site-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (2025.11.3)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.2 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (2.9.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in ./.venv/lib/python3.11/site-packages (from transformers[torch]) (1.12.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.2.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.1.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers[torch]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers[torch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers[torch]) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: underthesea in ./.venv/lib/python3.11/site-packages (8.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in ./.venv/lib/python3.11/site-packages (from underthesea) (8.3.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in ./.venv/lib/python3.11/site-packages (from underthesea) (0.9.11)\n",
            "Requirement already satisfied: nltk>=3.8 in ./.venv/lib/python3.11/site-packages (from underthesea) (3.9.2)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from underthesea) (2.32.5)\n",
            "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from underthesea) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in ./.venv/lib/python3.11/site-packages (from underthesea) (1.7.2)\n",
            "Requirement already satisfied: PyYAML in ./.venv/lib/python3.11/site-packages (from underthesea) (6.0.3)\n",
            "Requirement already satisfied: underthesea_core==1.0.5 in ./.venv/lib/python3.11/site-packages (from underthesea) (1.0.5)\n",
            "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.11/site-packages (from underthesea) (0.36.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk>=3.8->underthesea) (2025.11.3)\n",
            "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.6.1->underthesea) (2.3.5)\n",
            "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.6.1->underthesea) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.6.1->underthesea) (3.6.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub->underthesea) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->underthesea) (2025.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->underthesea) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->underthesea) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->underthesea) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->underthesea) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->underthesea) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->underthesea) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->underthesea) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/penguinsan/WorkSpace/Seminar/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#Install pip\n",
        "%pip install \"transformers[torch]\"\n",
        "%pip install \"underthesea\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split, DataLoader\n",
        "# from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModelForTokenClassification\n",
        "from underthesea import text_normalize, word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGoB5wYMPiE-"
      },
      "source": [
        "# 1. Ti·ªÅn x·ª≠ l√Ω text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2BM_lUGfEPB"
      },
      "outputs": [],
      "source": [
        "# Restored\n",
        "class VietnameseDiacriticRestorer:\n",
        "    def __init__(self, model_path='peterhung/vietnamese-accent-marker-xlm-roberta'):\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
        "        self.TOKENIZER_WORD_PREFIX = \"‚ñÅ\"\n",
        "\n",
        "        #device\n",
        "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        #load labels list\n",
        "        self.label_list = self._load_tags_set(\"selected_tags_names.txt\")\n",
        "\n",
        "    def insert_accents(self, text):\n",
        "      our_tokens = text.strip().split()\n",
        "\n",
        "      # the tokenizer may further split our tokens\n",
        "      inputs = self.tokenizer(our_tokens,\n",
        "                        is_split_into_words=True,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                        )\n",
        "\n",
        "      input_ids = inputs['input_ids']\n",
        "      tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "      tokens = tokens[1:-1]\n",
        "\n",
        "      with torch.no_grad():\n",
        "          inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "          outputs = self.model(**inputs)\n",
        "\n",
        "      predictions = outputs[\"logits\"].cpu().numpy()\n",
        "      predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "      # exclude output at index 0 and the last index, which correspond to '<s>' and '</s>'\n",
        "      predictions = predictions[0][1:-1]\n",
        "\n",
        "      return tokens, predictions\n",
        "\n",
        "    def _load_tags_set(self, fpath):\n",
        "      labels = []\n",
        "      with open(fpath, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                labels.append(line)\n",
        "      return labels\n",
        "\n",
        "    # assert len(label_list) == 528\n",
        "\n",
        "    def merge_tokens_and_preds(self, tokens, predictions):\n",
        "      merged_tokens_preds = []\n",
        "      i = 0\n",
        "      while i < len(tokens):\n",
        "        tok = tokens[i]\n",
        "        label_indexes = set([predictions[i]])\n",
        "        if tok.startswith(self.TOKENIZER_WORD_PREFIX): # start a new word\n",
        "            tok_no_prefix = tok[len(self.TOKENIZER_WORD_PREFIX):]\n",
        "            cur_word_toks = [tok_no_prefix]\n",
        "            # check if subsequent toks are part of this word\n",
        "            j = i + 1\n",
        "            while j < len(tokens):\n",
        "                if not tokens[j].startswith(self.TOKENIZER_WORD_PREFIX):\n",
        "                    cur_word_toks.append(tokens[j])\n",
        "                    label_indexes.add(predictions[j])\n",
        "                    j += 1\n",
        "                else:\n",
        "                    break\n",
        "            cur_word = ''.join(cur_word_toks)\n",
        "            merged_tokens_preds.append((cur_word, label_indexes))\n",
        "            i = j\n",
        "        else:\n",
        "            merged_tokens_preds.append((tok, label_indexes))\n",
        "            i += 1\n",
        "\n",
        "      return merged_tokens_preds\n",
        "\n",
        "    def get_accented_words(self, merged_tokens_preds, label_list):\n",
        "      accented_words = []\n",
        "      for word_raw, label_indexes in merged_tokens_preds:\n",
        "        # use the first label that changes word_raw\n",
        "        for label_index in label_indexes:\n",
        "            tag_name = label_list[int(label_index)]\n",
        "            raw, vowel = tag_name.split(\"-\")\n",
        "            if raw and raw in word_raw:\n",
        "                word_accented = word_raw.replace(raw, vowel)\n",
        "                break\n",
        "        else:\n",
        "            word_accented = word_raw\n",
        "\n",
        "        accented_words.append(word_accented)\n",
        "\n",
        "      return \" \".join(accented_words)\n",
        "\n",
        "    def restore(self, text):\n",
        "      tokens, predictions = self.insert_accents(text)\n",
        "      merged_tokens_preds = self.merge_tokens_and_preds(tokens, predictions)\n",
        "      accented_words = self.get_accented_words(merged_tokens_preds, self.label_list)\n",
        "      return accented_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBTchGMSlch5"
      },
      "outputs": [],
      "source": [
        "# Clear text\n",
        "class VietnameseTextStandardizer:\n",
        "    def __init__(self):\n",
        "        # T·ª´ ƒëi·ªÉn chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt/th√¥ng d·ª•ng\n",
        "        self.normalization_dict = {\n",
        "            \"sp\": \"s·∫£n ph·∫©m\", \"dk\": \"ƒë∆∞·ª£c\", \"dc\": \"ƒë∆∞·ª£c\", \"ko\": \"kh√¥ng\",\n",
        "            \"k\": \"kh√¥ng\", \"bt\": \"b√¨nh th∆∞·ªùng\", \"ok\": \"t·ªët\", \"oke\": \"t·ªët\",\n",
        "            \"okela\": \"t·ªët\", \"sg\": \"s√†i g√≤n\", \"hn\": \"h√† n·ªôi\", \"tks\": \"c·∫£m ∆°n\",\n",
        "            \"thank\": \"c·∫£m ∆°n\", \"please\": \"l√†m ∆°n\", \"thanks\": \"c·∫£m ∆°n\",\n",
        "            \"good\": \"t·ªët\", \"bad\": \"t·ªá\", \"very\": \"r·∫•t\", \"like\": \"th√≠ch\",\n",
        "            \"hate\": \"gh√©t\", \"du\": \"ƒë·ªß\"\n",
        "        }\n",
        "\n",
        "        # C√°c t·ª´ vi·∫øt li·ªÅn nhau\n",
        "        self.joined_words_dict = {\n",
        "            \"toithich\": \"t√¥i th√≠ch\",\n",
        "            \"toimuon\": \"t√¥i mu·ªën\",\n",
        "            \"toicamthay\": \"t√¥i c·∫£m th·∫•y\",\n",
        "            \"ratthich\": \"r·∫•t th√≠ch\",\n",
        "            \"quathich\": \"qu√° th√≠ch\",\n",
        "            \"thichqua\": \"th√≠ch qu√°\",\n",
        "            \"banthat\": \"b·∫°n th·∫≠t\",\n",
        "            \"spnay\": \"s·∫£n ph·∫©m n√†y\",\n",
        "            \"dichvunay\": \"d·ªãch v·ª• n√†y\",\n",
        "            \"toikhong\": \"t√¥i kh√¥ng\",\n",
        "            \"toiko\": \"t√¥i kh√¥ng\",\n",
        "            \"toicung\": \"t√¥i c≈©ng\",\n",
        "            \"toiratthich\": \"t√¥i r·∫•t th√≠ch\",  \n",
        "        }\n",
        "\n",
        "        # C√°c t·ª´ c√≥ k√≠ hi·ªáu emote\n",
        "        self.emoticon_sentiment_dict = {\n",
        "            # Positive emoticons\n",
        "            \":)\": \" t√≠ch_c·ª±c \", \":-)\": \" t√≠ch_c·ª±c \", \"=)\": \" t√≠ch_c·ª±c \",\n",
        "            \":D\": \" r·∫•t_t√≠ch_c·ª±c \", \":-D\": \" r·∫•t_t√≠ch_c·ª±c \", \"=D\": \" r·∫•t_t√≠ch_c·ª±c \",\n",
        "            \":)\": \" t√≠ch_c·ª±c \", \"üòä\": \" t√≠ch_c·ª±c \", \"üòç\": \" r·∫•t_t√≠ch_c·ª±c \",\n",
        "            \"ü§©\": \" r·∫•t_t√≠ch_c·ª±c \", \"üëç\": \" t·ªët \", \"‚ù§Ô∏è\": \" y√™u_th√≠ch \",\n",
        "            \"üíñ\": \" y√™u_th√≠ch \", \"üòò\": \" y√™u_th√≠ch \", \"ü•∞\": \" y√™u_th√≠ch \",\n",
        "            \"üòÅ\": \" vui \", \"üòÑ\": \" vui \", \"üòÜ\": \" vui \", \"üòÇ\": \" vui \",\n",
        "\n",
        "            # Negative emoticons\n",
        "            \":(\": \" ti√™u_c·ª±c \", \":-(\": \" ti√™u_c·ª±c \", \"=(\": \" ti√™u_c·ª±c \",\n",
        "            \":'(\": \" bu·ªìn \", \"üòû\": \" bu·ªìn \", \"üòî\": \" bu·ªìn \", \"üòü\": \" lo_l·∫Øng \",\n",
        "            \"üò†\": \" t·ª©c_gi·∫≠n \", \"üò°\": \" r·∫•t_t·ª©c_gi·∫≠n \", \"ü§¨\": \" r·∫•t_t·ª©c_gi·∫≠n \",\n",
        "            \"üëé\": \" t·ªá \", \"üíî\": \" th·∫•t_v·ªçng \", \"üò¢\": \" kh√≥c \", \"üò≠\": \" kh√≥c_nhi·ªÅu \",\n",
        "\n",
        "            # Neutral/Sarcastic\n",
        "            \":|\": \" b√¨nh_th∆∞·ªùng \", \":-|\": \" b√¨nh_th∆∞·ªùng \", \"üòê\": \" b√¨nh_th∆∞·ªùng \",\n",
        "            \"üòë\": \" kh√¥ng_h√†i_l√≤ng \", \"ü§®\": \" nghi_ng·ªù \", \"üòí\": \" ch√°n \",\n",
        "            \"üôÑ\": \" m·∫Øt_ƒë·∫£o \", \"üòè\": \" m·ªâa_mai \"\n",
        "        }\n",
        "\n",
        "        # üéØ EMOJI PATTERNS (ƒë·ªÉ kh√¥ng remove ho√†n to√†n)\n",
        "        self.emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"]+\", flags=re.UNICODE\n",
        "        )\n",
        "\n",
        "    def split_joined_words(self, text):\n",
        "        \"\"\"T√°ch t·ª´ vi·∫øt li·ªÅn b·∫±ng t·ª´ ƒëi·ªÉn\"\"\"\n",
        "        for joined_word, separated in self.joined_words_dict.items():\n",
        "            text = text.replace(joined_word, separated)\n",
        "        return text\n",
        "\n",
        "    def handle_emoticons(self, text):\n",
        "        \"\"\"Chuy·ªÉn emoticons th√†nh sentiment words\"\"\"\n",
        "        for emoticon, sentiment_word in self.emoticon_sentiment_dict.items():\n",
        "            text = text.replace(emoticon, sentiment_word)\n",
        "        return text\n",
        "\n",
        "    def standardize(self, text):\n",
        "        \"\"\"\n",
        "        Chu·∫©n h√≥a ti·∫øng Vi·ªát to√†n di·ªán\n",
        "        \"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # B∆∞·ªõc 1: Chu·∫©n h√≥a unicode & lowercase\n",
        "        normalized = text_normalize(text)\n",
        "\n",
        "        # B∆∞·ªõc 2: Chu·∫©n h√≥a kho·∫£ng tr·∫Øng\n",
        "        text = re.sub(r'\\s+', ' ', normalized).strip()\n",
        "\n",
        "        # B∆∞·ªõc 3: Chu·∫©n h√≥a emote\n",
        "        text = self.handle_emoticons(text)\n",
        "\n",
        "        # B∆∞·ªõc 4: T√°ch t·ª´ vi·∫øt li·ªÅn \n",
        "        text = self.split_joined_words(text)\n",
        "\n",
        "        # B∆∞·ªõc 5: T√°ch t·ª´ (QUAN TR·ªåNG)\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # B∆∞·ªõc 6: Chu·∫©n h√≥a t·ª´ v·ª±ng\n",
        "        standardized_tokens = []\n",
        "        for token in tokens:\n",
        "            # Chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt/th√¥ng d·ª•ng\n",
        "            standardized_token = self.normalization_dict.get(token.lower(), token.lower())\n",
        "            standardized_tokens.append(standardized_token)\n",
        "\n",
        "        # B∆∞·ªõc 7: Gh√©p l·∫°i th√†nh c√¢u chu·∫©n\n",
        "        clean_text = \" \".join(standardized_tokens)\n",
        "\n",
        "        return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-7xwv8rz5c7I"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis\n",
        "class VietnameseSentimentAnalyzer:\n",
        "    def __init__(self, model_name=\"wonrax/phobert-base-vietnamese-sentiment\"):\n",
        "        \"\"\"\n",
        "        model_name options:\n",
        "        - \"wonrax/phobert-base-vietnamese-sentiment\" (PhoBERT sentiment)\n",
        "        - \"vinai/phobert-base\" (PhoBERT base)\n",
        "        - \"FPTAI/vibert-base-cased\" (ViBERT)\n",
        "        \"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.sentiment_pipeline = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "\n",
        "        # Chu·∫©n h√≥a text\n",
        "        self.standardizer = VietnameseTextStandardizer()\n",
        "        self.restored = VietnameseDiacriticRestorer()\n",
        "\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Ph√¢n t√≠ch sentiment\"\"\"\n",
        "\n",
        "        # 1. Restored\n",
        "        restored_text = self.restored.restore(text)\n",
        "\n",
        "        # 2. Clear text\n",
        "        cleaned_text = self.standardizer.standardize(restored_text)\n",
        "\n",
        "        # 3. Ph√¢n t√≠ch sentiment b·∫±ng model ƒë√£ fine-tuned\n",
        "        result = self.sentiment_pipeline(cleaned_text)\n",
        "\n",
        "        return {\n",
        "            'original_text': text,\n",
        "            'cleaned_text': cleaned_text,\n",
        "            'sentiment': result[0]['label'],\n",
        "            'confidence': result[0]['score']\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AlUuzOsP1nt",
        "outputId": "200ee8f4-6b3a-45aa-fe11-4bccca6e49aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: toi that bai that roi\n",
            "Cleaned: t√¥i th·∫•t b·∫°i th·∫≠t r·ªìi\n",
            "Sentiment: NEG (Confidence: 0.9872)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# # S·ª≠ d·ª•ng\n",
        "analyzer = VietnameseSentimentAnalyzer()\n",
        "\n",
        "test_texts = [\n",
        "    \"toi that bai that roi\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    result = analyzer.analyze_sentiment(text)\n",
        "    print(f\"Text: {result['original_text']}\")\n",
        "    print(f\"Cleaned: {result['cleaned_text']}\")\n",
        "    print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.4f})\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
