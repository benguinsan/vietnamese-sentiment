"""
Module x·ª≠ l√Ω sentiment analysis cho ti·∫øng Vi·ªát
Ch·ª©a c√°c class: VietnameseDiacriticRestorer, VietnameseTextStandardizer, VietnameseSentimentAnalyzer
"""

import os
import re
import torch
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    pipeline,
    AutoModelForTokenClassification
)
from underthesea import text_normalize, word_tokenize

# ƒê∆∞·ªùng d·∫´n file tags (t·ª± ƒë·ªông t√¨m trong c√πng th∆∞ m·ª•c)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TAGS_FILE = os.path.join(BASE_DIR, "selected_tags_names.txt")


class VietnameseDiacriticRestorer:
    """Class ƒë·ªÉ restore d·∫•u ti·∫øng Vi·ªát cho text kh√¥ng d·∫•u"""
    
    def __init__(self, model_path='peterhung/vietnamese-accent-marker-xlm-roberta'):
        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)
        self.TOKENIZER_WORD_PREFIX = "‚ñÅ"

        # Device
        self.device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        self.model.to(self.device)
        self.model.eval()

        # Load labels list
        self.label_list = self._load_tags_set(TAGS_FILE)

    def insert_accents(self, text):
        """Insert accents v√†o text"""
        our_tokens = text.strip().split()

        # The tokenizer may further split our tokens
        inputs = self.tokenizer(
            our_tokens,
            is_split_into_words=True,
            truncation=True,
            padding=True,
            return_tensors="pt"
        )

        input_ids = inputs['input_ids']
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        tokens = tokens[1:-1]

        with torch.no_grad():
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.model(**inputs)

        predictions = outputs["logits"].cpu().numpy()
        predictions = np.argmax(predictions, axis=2)

        # Exclude output at index 0 and the last index, which correspond to '<s>' and '</s>'
        predictions = predictions[0][1:-1]

        return tokens, predictions

    def _load_tags_set(self, fpath):
        """Load tags t·ª´ file"""
        labels = []
        with open(fpath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    labels.append(line)
        return labels

    def merge_tokens_and_preds(self, tokens, predictions):
        """Merge tokens v√† predictions"""
        merged_tokens_preds = []
        i = 0
        while i < len(tokens):
            tok = tokens[i]
            label_indexes = set([predictions[i]])
            if tok.startswith(self.TOKENIZER_WORD_PREFIX):  # Start a new word
                tok_no_prefix = tok[len(self.TOKENIZER_WORD_PREFIX):]
                cur_word_toks = [tok_no_prefix]
                # Check if subsequent toks are part of this word
                j = i + 1
                while j < len(tokens):
                    if not tokens[j].startswith(self.TOKENIZER_WORD_PREFIX):
                        cur_word_toks.append(tokens[j])
                        label_indexes.add(predictions[j])
                        j += 1
                    else:
                        break
                cur_word = ''.join(cur_word_toks)
                merged_tokens_preds.append((cur_word, label_indexes))
                i = j
            else:
                merged_tokens_preds.append((tok, label_indexes))
                i += 1

        return merged_tokens_preds

    def get_accented_words(self, merged_tokens_preds, label_list):
        """Get accented words t·ª´ merged tokens v√† predictions"""
        accented_words = []
        for word_raw, label_indexes in merged_tokens_preds:
            # Use the first label that changes word_raw
            word_accented = word_raw
            for label_index in label_indexes:
                tag_name = label_list[int(label_index)]
                raw, vowel = tag_name.split("-")
                if raw and raw in word_raw:
                    word_accented = word_raw.replace(raw, vowel)
                    break

            accented_words.append(word_accented)

        return " ".join(accented_words)

    def restore(self, text):
        """Restore d·∫•u cho text"""
        tokens, predictions = self.insert_accents(text)
        merged_tokens_preds = self.merge_tokens_and_preds(tokens, predictions)
        accented_words = self.get_accented_words(merged_tokens_preds, self.label_list)
        return accented_words


class VietnameseTextStandardizer:
    """Class ƒë·ªÉ chu·∫©n h√≥a text ti·∫øng Vi·ªát"""
    
    def __init__(self):
        # T·ª´ ƒëi·ªÉn chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt/th√¥ng d·ª•ng
        self.normalization_dict = {
            "sp": "s·∫£n ph·∫©m", "dk": "ƒë∆∞·ª£c", "dc": "ƒë∆∞·ª£c", "ko": "kh√¥ng",
            "k": "kh√¥ng", "bt": "b√¨nh th∆∞·ªùng", "ok": "t·ªët", "oke": "t·ªët",
            "okela": "t·ªët", "sg": "s√†i g√≤n", "hn": "h√† n·ªôi", "tks": "c·∫£m ∆°n",
            "thank": "c·∫£m ∆°n", "please": "l√†m ∆°n", "thanks": "c·∫£m ∆°n",
            "good": "t·ªët", "bad": "t·ªá", "very": "r·∫•t", "like": "th√≠ch",
            "hate": "gh√©t", "du": "ƒë·ªß"
        }

        # C√°c t·ª´ vi·∫øt li·ªÅn nhau
        self.joined_words_dict = {
            "toithich": "t√¥i th√≠ch",
            "toimuon": "t√¥i mu·ªën",
            "toicamthay": "t√¥i c·∫£m th·∫•y",
            "ratthich": "r·∫•t th√≠ch",
            "quathich": "qu√° th√≠ch",
            "thichqua": "th√≠ch qu√°",
            "banthat": "b·∫°n th·∫≠t",
            "spnay": "s·∫£n ph·∫©m n√†y",
            "dichvunay": "d·ªãch v·ª• n√†y",
            "toikhong": "t√¥i kh√¥ng",
            "toiko": "t√¥i kh√¥ng",
            "toicung": "t√¥i c≈©ng",
            "toiratthich": "t√¥i r·∫•t th√≠ch",
        }

        # C√°c t·ª´ c√≥ k√≠ hi·ªáu emote
        self.emoticon_sentiment_dict = {
            # Positive emoticons
            ":)": " t√≠ch_c·ª±c ", ":-)": " t√≠ch_c·ª±c ", "=)": " t√≠ch_c·ª±c ",
            ":D": " r·∫•t_t√≠ch_c·ª±c ", ":-D": " r·∫•t_t√≠ch_c·ª±c ", "=D": " r·∫•t_t√≠ch_c·ª±c ",
            "üòä": " t√≠ch_c·ª±c ", "üòç": " r·∫•t_t√≠ch_c·ª±c ",
            "ü§©": " r·∫•t_t√≠ch_c·ª±c ", "üëç": " t·ªët ", "‚ù§Ô∏è": " y√™u_th√≠ch ",
            "üíñ": " y√™u_th√≠ch ", "üòò": " y√™u_th√≠ch ", "ü•∞": " y√™u_th√≠ch ",
            "üòÅ": " vui ", "üòÑ": " vui ", "üòÜ": " vui ", "üòÇ": " vui ",

            # Negative emoticons
            ":(": " ti√™u_c·ª±c ", ":-(": " ti√™u_c·ª±c ", "=(": " ti√™u_c·ª±c ",
            ":'(": " bu·ªìn ", "üòû": " bu·ªìn ", "üòî": " bu·ªìn ", "üòü": " lo_l·∫Øng ",
            "üò†": " t·ª©c_gi·∫≠n ", "üò°": " r·∫•t_t·ª©c_gi·∫≠n ", "ü§¨": " r·∫•t_t·ª©c_gi·∫≠n ",
            "üëé": " t·ªá ", "üíî": " th·∫•t_v·ªçng ", "üò¢": " kh√≥c ", "üò≠": " kh√≥c_nhi·ªÅu ",

            # Neutral/Sarcastic
            ":|": " b√¨nh_th∆∞·ªùng ", ":-|": " b√¨nh_th∆∞·ªùng ", "üòê": " b√¨nh_th∆∞·ªùng ",
            "üòë": " kh√¥ng_h√†i_l√≤ng ", "ü§®": " nghi_ng·ªù ", "üòí": " ch√°n ",
            "üôÑ": " m·∫Øt_ƒë·∫£o ", "üòè": " m·ªâa_mai "
        }

        # EMOJI PATTERNS (ƒë·ªÉ kh√¥ng remove ho√†n to√†n)
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "]+", flags=re.UNICODE
        )

    def split_joined_words(self, text):
        """T√°ch t·ª´ vi·∫øt li·ªÅn b·∫±ng t·ª´ ƒëi·ªÉn"""
        for joined_word, separated in self.joined_words_dict.items():
            text = text.replace(joined_word, separated)
        return text

    def handle_emoticons(self, text):
        """Chuy·ªÉn emoticons th√†nh sentiment words"""
        for emoticon, sentiment_word in self.emoticon_sentiment_dict.items():
            text = text.replace(emoticon, sentiment_word)
        return text

    def standardize(self, text):
        """
        Chu·∫©n h√≥a ti·∫øng Vi·ªát to√†n di·ªán
        """
        if not text or not isinstance(text, str):
            return ""

        # B∆∞·ªõc 1: Chu·∫©n h√≥a unicode & lowercase
        normalized = text_normalize(text)

        # B∆∞·ªõc 2: Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', normalized).strip()

        # B∆∞·ªõc 3: Chu·∫©n h√≥a emote
        text = self.handle_emoticons(text)

        # B∆∞·ªõc 4: T√°ch t·ª´ vi·∫øt li·ªÅn
        text = self.split_joined_words(text)

        # B∆∞·ªõc 5: T√°ch t·ª´ (QUAN TR·ªåNG)
        tokens = word_tokenize(text)

        # B∆∞·ªõc 6: Chu·∫©n h√≥a t·ª´ v·ª±ng
        standardized_tokens = []
        for token in tokens:
            # Chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt/th√¥ng d·ª•ng
            standardized_token = self.normalization_dict.get(token.lower(), token.lower())
            standardized_tokens.append(standardized_token)

        # B∆∞·ªõc 7: Gh√©p l·∫°i th√†nh c√¢u chu·∫©n
        clean_text = " ".join(standardized_tokens)

        return clean_text


class VietnameseSentimentAnalyzer:
    """Class ch√≠nh ƒë·ªÉ ph√¢n t√≠ch sentiment ti·∫øng Vi·ªát"""
    
    def __init__(self, model_name="wonrax/phobert-base-vietnamese-sentiment"):
        """
        Kh·ªüi t·∫°o Vietnamese Sentiment Analyzer
        
        Args:
            model_name: T√™n model t·ª´ HuggingFace
                - "wonrax/phobert-base-vietnamese-sentiment" (PhoBERT sentiment) - Default
                - "vinai/phobert-base" (PhoBERT base)
                - "FPTAI/vibert-base-cased" (ViBERT)
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model=self.model,
            tokenizer=self.tokenizer
        )

        # Chu·∫©n h√≥a text
        self.standardizer = VietnameseTextStandardizer()
        self.restored = VietnameseDiacriticRestorer()

    def analyze_sentiment(self, text):
        """
        Ph√¢n t√≠ch sentiment cho text ti·∫øng Vi·ªát
        
        Args:
            text: Text c·∫ßn ph√¢n t√≠ch
            
        Returns:
            dict: {
                'original_text': text g·ªëc,
                'text': text ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω,
                'sentiment': label (NEG/POS/NEU),
                'confidence': confidence score (0-1)
                'all_scores': scores cho t·∫•t c·∫£ labels
            }
        """
        # 1. Restore d·∫•u
        restored_text = self.restored.restore(text)

        # 2. Chu·∫©n h√≥a text
        cleaned_text = self.standardizer.standardize(restored_text)

        # 3. Ph√¢n t√≠ch sentiment b·∫±ng model ƒë√£ fine-tuned
        # L·∫•y t·∫•t c·∫£ scores ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß
        result = self.sentiment_pipeline(cleaned_text, return_all_scores=True)
        
        # T·∫°o dictionary scores cho t·∫•t c·∫£ labels
        all_scores = {}
        for item in result[0]:
            all_scores[item['label']] = item['score']
        
        # L·∫•y label c√≥ score cao nh·∫•t
        top_result = max(result[0], key=lambda x: x['score'])

        return {
            'original_text': text,
            'text': cleaned_text,
            'sentiment': top_result['label'],
            'confidence': top_result['score'],
            'all_scores': all_scores 
        }